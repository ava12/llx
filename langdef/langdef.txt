# Token type definitions, except for external types.

# Every byte of a source text must be a part of some token,
# otherwise a "wrong character" error is raised.

# Regexp body uses RE2 syntax.
# Note that "." matches any character including newline ("s" flag is used).
# Order of declarations matters: earlier ones are preferred.
# Definitions must not contain capturing groups.

# Every defined token type will be added to 0-based token list
# in order of appearance. E.g. 5 token types will have indexes 0-4.

$space = /[ \r\n\t\f]+/;
$comment = /#.*?(?:\n|$)/;
$string = /(?:".*?")|(?:'.*?')/;
$name = /[a-zA-z_][a-zA-Z_0-9-]*/;
$type-dir = /!(?:aside|caseless|error|extern|group|shrink)\b/;
$literal-dir = /!literal\b/;
$token-name = /\$[a-zA-z_][a-zA-Z_0-9-]*/;
$regexp = /\/(?:[^\\\/]|\\.)+\//;
$op = /[(){}\[\]=|,;]/;
$error = /["'!].{0,10}/;


# Recognized directives, defaults are empty lists.
# Each directive takes a list of space-separated token
# types (in form $type) or literals ("text" or 'text').
# Directives may precede corresponding token type definitions.

# Token types that may be found in source text, but are not used
# in syntax rules. May be hooked. Unhandled aside tokens are
# not emitted to parser.
!aside $space $comment;

# Case-insensitive token types, e.g. SQL keywords. Parser converts
# caseless token text to uppercase before comparing it with a literal.
!caseless ;

# "Token" types indicating lexical errors, e.g. unmatched opening quote.
# Used for more descriptive error messages.
!error $error;

# Token types that are not defined in grammar, but may be emitted by hooks.
# E.g. for Python it could be feasible to track current indentation level
# and emit fake $begin or $end when it changes.
# External token types will be added to token list after defined types
# and before literal tokens, in order of appearance.
!extern ;

# Define a separate group of token types. Empty lists are ignored.
# Each directive defines a new group, all remaining tokens form
# "default" group. A token type can belong to different groups.
# Literal token groups are detected automatically.
# Each group effectively constructs a separate lexer. This can be used
# when tokenization result depends on context.
# E.g. parsing a HTML file would require at least two groups: one consisting
# of raw text and "<" tokens and another one containing parts of HTML tags.
# All tokens allowed at some parsing state must belong to the same group.
!group ;

# Define literal tokens. Every literal must match at least one token type.
# All literals will be added to the end of token list after defined
# and external token types, in order of appearance.
# All literal tokens used in grammar rules but not listed with this directive
# will be added to the end of token list.
!literal ;

# Token types that can be split into smaller parts.
# By default lexer captures the longest possible character sequence
# that matches a valid token, but sometimes it should capture a shorter one.
# E.g. in C++ templates ">>" sequence should be split in ">" operators.
!shrink ;


# Non-terminal definitions. The first one is the root.
# Definitions may contain string literals. All new literals (not listed in
# !literal directive) will be added to token list in order of appearance.
# When parser encounters an ambiguous rule, i.e. two or more variants have
# the same prefix, the one that consumes the longest run of tokens is chosen.

langdef = {directive | token-definition}, definition, {definition};
directive = type-directive | literal-directive;
type-directive = $type-dir, {$token-name}, ';';
literal-directive = $literal-dir, {$string}, ';';
token-definition = $token-name, '=', $regexp, ';';
definition = $name, '=', sequence, ';';
sequence = item, {',', item};
item = variant, {'|', variant};
variant = $name | $token-name | $string | group | optional | repeat;
group = '(', sequence, ')';
optional = '[', sequence, ']';
repeat = '{', sequence, '}';
